{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import os \n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "import os\n",
    "import errno\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell contains function to convert ILL raw data to the forseen Loki file format\n",
    "\n",
    "def loki_file_creator(loki_path, loki_filename):\n",
    "    \"\"\"\n",
    "    This function creates a dummy loki_basic.nxs that has the expected file structure for ESS Loki.\n",
    "    Afterwards, I will append the nurf data and rename the file.\n",
    "    \"\"\"\n",
    "    with h5py.File(os.path.join(loki_path,loki_filename), 'w') as hf:\n",
    "        nxentry = hf.create_group(\"entry\")\n",
    "        nxentry.attrs['NX_class'] = 'NXentry'\n",
    "        nxinstrument=nxentry.create_group(\"instrument\") \n",
    "        nxinstrument.attrs['NX_class'] = 'NXinstrument'\n",
    "\n",
    "\n",
    "def load_one_spectro_file(file_handle, path_rawdata):\n",
    "    \"\"\"\n",
    "    This function loads one .nxs file containing spectroscopy data (fluo, uv). Data is stored in multiple np.ndarrays.\n",
    "\n",
    "    In:\n",
    "     file_handle: file_handle is a file number, one element is expected otherwise an error is raised\n",
    "                type: list\n",
    "\n",
    "    path_rawdata: Path to the raw data\n",
    "                type: str\n",
    "\n",
    "    Out:\n",
    "        data: contains all relevant HDF5 entries and their content for the Nurf project (keys and values)\n",
    "            type: dict\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # create path to file, convert file_number to string\n",
    "    file_path_spectro = os.path.join(path_rawdata, file_handle + '.nxs')\n",
    "\n",
    "    # check if file exists\n",
    "    if not os.path.isfile(file_path_spectro):\n",
    "        raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT),\n",
    "                                file_path_spectro)\n",
    "\n",
    "    # we are ready to load the data set\n",
    "    # open the .nxs file and read the values\n",
    "    with h5py.File(file_path_spectro, \"r\") as f:\n",
    "        # access nurf sub-group\n",
    "        nurf_group = '/entry0/D22/nurf/'\n",
    "\n",
    "        # access keys in sub-group\n",
    "        nurf_keys = list(f[nurf_group].keys())\n",
    "        # how many keys exist\n",
    "        len_nurf_keys = len(nurf_keys)\n",
    "\n",
    "        # print(nurf_keys)\n",
    "        # print(len_nurf_keys)\n",
    "\n",
    "        # this returns a list with HDF5datasets (I think)\n",
    "        # data_spectro_file=list(f[nurf_group].values())\n",
    "\n",
    "        # data_spectro_file=f[nurf_group].values()\n",
    "        # data_spectro_file=f[nurf_group]\n",
    "\n",
    "        # extract all data of the nurf subgroup and store it in a new dict\n",
    "\n",
    "        # initialise an empty dict\n",
    "        data = {}\n",
    "\n",
    "        for key in f[nurf_group].keys():\n",
    "            # print(key)\n",
    "            # this is how I get string giving the full path to this dataset\n",
    "            path_dataset = f[nurf_group][key].name\n",
    "            # print(path_dataset)\n",
    "            # print(f[nurf_group][key].name) #this prints the full path to the dataset\n",
    "            # print(type(f[path_dataset][:])) #this gives me access to the content of the data set, I could use : or () inside [], out: np.ndarray\n",
    "\n",
    "            # This gives a dict with full path name as dict entry followed by value. No, I don't want this, but good to know.\n",
    "            # data[f[nurf_group][key].name]=f[path_dataset][:]\n",
    "\n",
    "            # This gives a dict where the keys corresponds to the key names of the h5 file.\n",
    "            data[key] = f[path_dataset][:]\n",
    "\n",
    "        # print(f[nurf_group].get('Fluo_spectra'))\n",
    "\n",
    "        # print a hierachical view of the file (simple)\n",
    "        # like this is would only go through subgroups\n",
    "        # f[nurf_group].visititems(lambda x, y: print(x))\n",
    "\n",
    "        # walk through the whole file and show the attributes (found on github as an explanation for this function)\n",
    "        # def print_attrs(name, obj):\n",
    "        #    print(name)\n",
    "        #    for key, val in obj.attrs.items():\n",
    "        #        print(\"{0}: {1}\".format(key, val))\n",
    "        # f[nurf_group].visititems(print_attrs)\n",
    "\n",
    "        # print(data_spectro_file)\n",
    "\n",
    "    # file_handle is returned as np.ndarray and its an np.array, elements correspond to row indices\n",
    "    return data\n",
    "\n",
    "\n",
    "def nurf_file_creator(loki_file, path_to_loki_file, data):\n",
    "    \"\"\"\n",
    "    Appends NUrF group to LOKI NeXus file for ESS\n",
    "\n",
    "    Args:\n",
    "        loki_file (str): filename of NeXus file for Loki\n",
    "        path_to_loki_file (str): File path where the NeXus file for LOKI is stored\n",
    "        data (dict): Dictionary with dummy data for Nurf\n",
    "    \"\"\"\n",
    "\n",
    "    # change directory where the loki.nxs is located\n",
    "    os.chdir(path_to_loki_file)\n",
    "\n",
    "    # open the file and append\n",
    "    with h5py.File(loki_file, 'a') as hf:\n",
    "                \n",
    "        #comment on names\n",
    "        #UV/FL_Background is the dark\n",
    "        #UV/FL_Intensity0 is the reference\n",
    "        #UV/FL_Spectra is the sample\n",
    "        \n",
    "        # image_key: number of frames (nFrames) given indirectly as part of the shape of the arrays \n",
    "        # TODO: keep in mind what happens if multiple dark or reference frames are taken\n",
    "        \n",
    "        # remove axis=2 of length one from this array\n",
    "        data['UV_spectra']=np.squeeze(data['UV_spectra'], axis=2)  #this removes the third axis in this array, TODO: needs later to be verified with real data from hardware\n",
    "        \n",
    "        # assemble all spectra in one variable\n",
    "        uv_all_data=np.row_stack((data['UV_spectra'],data['UV_background'], data['UV_intensity0']))\n",
    "      \n",
    "        dummy_vec=np.full(np.shape(uv_all_data)[0], False) \n",
    "\n",
    "        #create boolean masks for data, dark, reference\n",
    "        uv_nb_spectra=np.shape(data['UV_spectra'])[0]\n",
    "\n",
    "        # data mask, copy and replace first entries \n",
    "        uv_data_mask=copy.copy(dummy_vec)\n",
    "        uv_data_mask[0:uv_nb_spectra]=True\n",
    "\n",
    "        # dark mask\n",
    "        # find out how many darks exist\n",
    "        # TODO: Is there always a background or do we need to catch this case if there isn't?\n",
    "        if data['UV_background'].ndim==1:\n",
    "            uv_nb_darks=1\n",
    "        else: \n",
    "            uv_nb_darks=np.shape(data['UV_background'])[1]  #TODO: needs to be verified with real data from Judith's setup\n",
    "\n",
    "        uv_dark_mask=copy.copy(dummy_vec)\n",
    "        uv_dark_mask[uv_nb_spectra:uv_nb_spectra+uv_nb_darks]=True\n",
    "\n",
    "        # reference \n",
    "        # how many references where taken? \n",
    "        if data['UV_intensity0'].ndim==1:\n",
    "            uv_nb_ref=1\n",
    "        else:\n",
    "            uv_nb_ref=np.shape(data['UV_intensity0'])[1]  #TODO: needs to be verified with real data from Judith's setup\n",
    "        \n",
    "        # reference mask, copy and replace first entries \n",
    "        uv_ref_mask=copy.copy(dummy_vec)\n",
    "        uv_ref_mask[uv_nb_spectra+uv_nb_darks:uv_nb_spectra+uv_nb_darks+uv_nb_ref]=True\n",
    "\n",
    "        \n",
    "        \n",
    "        # UV subgroup\n",
    "        grp_uv = hf.create_group(\"/entry/instrument/uv\")\n",
    "        grp_uv.attrs[\"NX_class\"] = 'NXdata'\n",
    "        \n",
    "        # uv spectra\n",
    "        uv_signal_data=grp_uv.create_dataset('data', data=uv_all_data, dtype=np.float32)\n",
    "        uv_signal_data.attrs['long name']= 'all_data'\n",
    "        uv_signal_data.attrs['units']= 'counts'\n",
    "        grp_uv.attrs['signal']= 'data'  #indicate that the main signal is data \n",
    "        grp_uv.attrs['axes']= [ \"spectrum\", \"wavelength\" ] #time is here the first axis, i.e axis=0, wavelength is axis=1\n",
    "        \n",
    "        # define the AXISNAME_indices\n",
    "        grp_uv.attrs['time_indices'] = 0\n",
    "        grp_uv.attrs['integration_time_indices'] = 0\n",
    "        grp_uv.attrs['wavelength_indices'] = 1\n",
    "\n",
    "        # introducing a key that is interpretable for sample, dark, and reference \n",
    "        grp_uv.attrs['is_sample_indices'] = 0\n",
    "        grp_uv.attrs['is_dark_indices'] = 0\n",
    "        grp_uv.attrs['is_reference_indices'] = 0  \n",
    "\n",
    "        grp_uv.create_dataset('is_sample', data=uv_data_mask, dtype=bool)\n",
    "        grp_uv.create_dataset('is_dark', data=uv_dark_mask, dtype=bool)\n",
    "        grp_uv.create_dataset('is_reference', data=uv_ref_mask, dtype=bool)\n",
    "\n",
    "        # uv_time\n",
    "        # dummy timestamps for uv_time\n",
    "        # TODO: Codes will have to change later for the real hardware.\n",
    "        uv_time = np.empty(np.shape(uv_all_data)[0], dtype='datetime64[us]')  \n",
    "        for i in range(0, np.shape(uv_time)[0]):\n",
    "            uv_time[i]=np.datetime64('now')\n",
    "    \n",
    "        # see https://stackoverflow.com/questions/23570632/store-datetimes-in-hdf5-with-h5py \n",
    "        # suggested work around because h5py does not support time types\n",
    "        uv_time_data=grp_uv.create_dataset('time', data=uv_time.view('<i8'), dtype='<i8')\n",
    "        uv_time_data.attrs['unit'] = 'us'\n",
    "        # to read\n",
    "        #print(uv_time_data[:].view('<M8[us]'))\n",
    "        # TODO: Do we need here an attribute for the unit?\n",
    "       \n",
    "        # uv_wavelength\n",
    "        uv_wavelength_data=grp_uv.create_dataset('wavelength', data=data['UV_wavelength'], dtype=np.float32)\n",
    "        \n",
    "        uv_wavelength_data.attrs['units'] = 'nm'  # TODO: unit to be verified\n",
    "        uv_wavelength_data.attrs['long name'] = 'wavelength'\n",
    "                \n",
    "                \n",
    "        # creating for each spectrum, even dark and background, the integration time\n",
    "        # TODO: needs to be verified with real hardware\n",
    "        uv_inttime=np.full(np.shape(uv_all_data)[0],data['UV_IntegrationTime'])\n",
    "        \n",
    "         # uv_integration_time\n",
    "        uv_inttime_data=grp_uv.create_dataset(\"integration_time\",data=uv_inttime, dtype=np.int32)\n",
    "                   \n",
    "        uv_inttime_data.attrs['long_name'] = 'integration_time'\n",
    "        uv_inttime_data.attrs['units'] = 'us'  # TODO: unit to be verified, currently in micro-seconds\n",
    "\n",
    "        # Fluorescence subgroup\n",
    "        grp_fluo = hf.create_group(\"/entry/instrument/fluorescence\")\n",
    "        grp_fluo.attrs[\"NX_class\"] = 'NXdata'\n",
    "        \n",
    "        # currenty real fluo data is often messed up (i.e. empty spectra inbetween real ones)\n",
    "        # remove third axis of length one\n",
    "        data['Fluo_spectra']=np.squeeze(data['Fluo_spectra'], axis=2)\n",
    "\n",
    "        # Something is not okay with the real Fluo_intensity0 data from ILL. It contains only one 0, at least in my file from the ILL beamtime. \n",
    "        # Also, some fluo spcetra in between are just dummy ones. There were acquisition problems.\n",
    "        # This code block can later be adjusted\n",
    "        try:\n",
    "            assert (np.shape(data['Fluo_intensity0'])[0]!=1), 'Fluo_intensity0 contains only one value.'\n",
    "        except AssertionError as error:\n",
    "            data['Fluo_intensity0']=data['Fluo_intensity0'][0]*np.ones((np.shape(data['Fluo_background'])[0]))\n",
    "           \n",
    "        # assemble all fluo data    \n",
    "        fluo_all_data=np.row_stack((data['Fluo_spectra'],data['Fluo_background'], data['Fluo_intensity0']))\n",
    "\n",
    "        # dummy vec for mask\n",
    "        dummy_vec=np.full(np.shape(fluo_all_data)[0], False) \n",
    "\n",
    "        # how many fluo data spectra\n",
    "        fluo_nb_spectra=np.shape(data['Fluo_spectra'])[0]\n",
    "        \n",
    "        #create boolean masks for data, dark, reference\n",
    "        # data mask, copy and replace first entries \n",
    "        fluo_data_mask=copy.copy(dummy_vec)\n",
    "        fluo_data_mask[0:fluo_nb_spectra]=True\n",
    "\n",
    "        #how many backgrounds?\n",
    "        if data['Fluo_background'].ndim==1:\n",
    "            fluo_nb_dark=1\n",
    "        else:\n",
    "            fluo_nb_dark=np.shape(data['Fluo_background'])[1] #TODO: needs to be verified with Judith's setup\n",
    "        fluo_dark_mask=copy.copy(dummy_vec)\n",
    "        fluo_dark_mask[fluo_nb_spectra:fluo_nb_spectra+fluo_nb_dark]=True\n",
    "\n",
    "        #how many references? \n",
    "        if data['Fluo_intensity0'].ndim==1:\n",
    "            fluo_nb_ref=1\n",
    "        else:\n",
    "            fluo_nb_ref=np.shape(data['Fluo_intensity0'])[1] #TODO: needs to be verified with Judith's setup\n",
    "\n",
    "        fluo_ref_mask=copy.copy(dummy_vec)\n",
    "        fluo_ref_mask[fluo_nb_spectra+fluo_nb_dark:fluo_nb_spectra+fluo_nb_dark+fluo_nb_ref]=True\n",
    "                \n",
    "        \n",
    "        # maybe it does not matter at what monowavelength dark and reference are measured, but I have to add then dummy values\n",
    "        # dummy value for monowavelength for dark:  NaN nm\n",
    "        # dummy value for monowavelength for reference (until I know):  NaN nm\n",
    "        # again the fluo data can be messed up here, example: 1 monowavelength, but 7 fluo spectra in  103418.nxs\n",
    "        try:\n",
    "            assert (np.shape(data['Fluo_monowavelengths'])[0]!=1), 'Fluo_monowavelengths contains only one value.'\n",
    "        except AssertionError as error:\n",
    "            data['Fluo_monowavelengths']=data['Fluo_monowavelengths'][0]*np.zeros((np.shape(data['Fluo_spectra'])[0]))\n",
    "        \n",
    "        #how many monowavelengths ?  \n",
    "        nb_monowavelengths=data['Fluo_monowavelengths'].size\n",
    "        \n",
    "        # create a dummy vector for monowavelength \n",
    "        dummy_mwl=np.ones(dummy_vec.shape)*(-1)\n",
    "        # I replace the first entries with the corect Fluo_monowavelengths\n",
    "        dummy_mwl[0:nb_monowavelengths]=data['Fluo_monowavelengths']\n",
    "        # now I need to add the monowavelengths for dark and reference, but they are not stored with the ILL data :(\n",
    "        # I go for NaN\n",
    "        # add monowavelengths for number of darks\n",
    "        dummy_mwl[nb_monowavelengths:nb_monowavelengths+fluo_nb_dark]=np.nan\n",
    "        # add monowavelengths for number of reference\n",
    "        dummy_mwl[nb_monowavelengths+fluo_nb_dark:nb_monowavelengths+fluo_nb_dark+fluo_nb_ref]=np.nan\n",
    "        \n",
    "        data['Fluo_monowavelengths']=dummy_mwl\n",
    "\n",
    "        # fluo spectra\n",
    "        fluo_signal_data = grp_fluo.create_dataset('data',\n",
    "                                               data=fluo_all_data, dtype=np.float32)\n",
    "        fluo_signal_data.attrs['long name'] = 'all_data'\n",
    "        fluo_signal_data.attrs['units'] = 'counts'\n",
    "    \n",
    "\n",
    "        grp_fluo.attrs['signal']= 'data'  #indicate that the main signal is data \n",
    "        grp_fluo.attrs['axes']= [ \"spectrum\",  \"wavelength\"]\n",
    "        \n",
    "        # define the AXISNAME_indices\n",
    "        grp_fluo.attrs['time_indices'] = 0\n",
    "        grp_fluo.attrs['integration_time_indices'] = 0\n",
    "        grp_fluo.attrs['monowavelengths_indices'] = 0\n",
    "        grp_fluo.attrs['wavelength_indices'] = 1\n",
    "        \n",
    "\n",
    "        grp_fluo.attrs['is_sample_indices'] = 0\n",
    "        grp_fluo.attrs['is_dark_indices'] = 0\n",
    "        grp_fluo.attrs['is_reference_indices'] = 0  \n",
    "\n",
    "        grp_fluo.create_dataset('is_sample',data=fluo_data_mask,dtype=bool)\n",
    "        grp_fluo.create_dataset('is_dark',data=fluo_dark_mask,dtype=bool)\n",
    "        grp_fluo.create_dataset('is_reference',data=fluo_ref_mask,dtype=bool)\n",
    "\n",
    "       \n",
    "        # fluo_time\n",
    "        # dummy timestamps for fluo_time\n",
    "        # TODO: Codes will have to change later for the real hardware.\n",
    "        fluo_time = np.empty(np.shape(fluo_all_data)[0], dtype='datetime64[us]')  \n",
    "        for i in range(0, np.shape(fluo_time)[0]):\n",
    "            fluo_time[i]=np.datetime64('now')\n",
    "    \n",
    "        # see https://stackoverflow.com/questions/23570632/store-datetimes-in-hdf5-with-h5py \n",
    "        # suggested work around because h5py does not support time types\n",
    "        fluo_time_data=grp_fluo.create_dataset('time', data=fluo_time.view('<i8'), dtype='<i8')\n",
    "        fluo_time_data.attrs['units'] = 'us'\n",
    "        # to read\n",
    "        #print(fluo_time_data[:].view('<M8[us]'))\n",
    "        # TODO: Do we need here an attribute for the unit?\n",
    "       \n",
    "       \n",
    "        # creating integration time for each fluo spectrum including dark and reference\n",
    "        # TODO: needs to be verfied with hardware\n",
    "        fluo_inttime=np.full(np.shape(fluo_all_data)[0],data['Fluo_IntegrationTime'])\n",
    "       \n",
    "        # fluo_integration_time\n",
    "        fluo_inttime_data = grp_fluo.create_dataset('integration_time',\n",
    "                                               data=fluo_inttime,\n",
    "                                               dtype=np.float32)\n",
    "        fluo_inttime_data.attrs['units'] = 'us'  # TODO: unit to be verified, currently micro-seconds\n",
    "        fluo_inttime_data.attrs['long name'] = 'integration_time'\n",
    "\n",
    "\n",
    "        # fluo_monowavelengths\n",
    "        fluo_monowavelengths_data = grp_fluo.create_dataset('monowavelengths',\n",
    "                                                       data=data[\n",
    "                                                           'Fluo_monowavelengths'],\n",
    "                                                       dtype=np.float32)\n",
    "        fluo_monowavelengths_data.attrs['units'] = 'nm'  # TODO: unit to be verified\n",
    "        fluo_monowavelengths_data.attrs['long name'] = 'monowavelengths'\n",
    "\n",
    "        \n",
    "        # fluo_wavelength\n",
    "        fluo_wavelength_data= grp_fluo.create_dataset('wavelength',\n",
    "                                                  data=data['Fluo_wavelength'],\n",
    "                                                  dtype=np.float32)\n",
    "        fluo_wavelength_data.attrs['units'] = 'nm'  # TODO: unit to be verified\n",
    "        fluo_wavelength_data.attrs['long name'] = 'fluo_wavelength'\n",
    "\n",
    "        # dummy groups, no information currently available\n",
    "        #grp_sample_cell = hf.create_group(\"/entry/sample/sample_cell\")\n",
    "        #grp_sample_cell.attrs[\"NX_class\"] = 'NXenvironment'\n",
    "        #grp_sample_cell.create_dataset('description', data='NUrF sample cell')\n",
    "        #grp_sample_cell.create_dataset('type', data='SQ1-ALL')\n",
    "\n",
    "        #grp_pumps = hf.create_group(\"/entry/sample/hplc_pump\")\n",
    "        #grp_pumps.attrs[\"NX_class\"] = 'NXenvironment'\n",
    "        #grp_pumps.create_dataset(\"description\", data='HPLC_pump')\n",
    "\n",
    "        #no more valves\n",
    "        #grp_valves = grp_nurf.create_group(\"Valves\")\n",
    "        #grp_valves.attrs[\"NX_class\"] = 'NXenvironment'\n",
    "        #grp_valves.create_dataset(\"description\", data='Valves')\n",
    "\n",
    "        #grp_densito = hf.create_group(\"/entry/instrument/densitometer\")\n",
    "        #grp_densito.attrs[\"NX_class\"] = 'NXdetector'\n",
    "        #grp_densito.create_dataset(\"description\", data='Densitometer')\n",
    "\n",
    "def convert_ill2loki(scan_numbers, path_to_loki_file, loki_file, path_rawdata, process_folder):\n",
    "    \"\"\"\n",
    "    Convert ill data sets of files to new LoKI data structure.\n",
    "    scan_numbers: list of filenumbers without trailing zero\n",
    "    path_to_loki_file: path to LOKI_basics.nxs\n",
    "    loki_file:  LOKI_Baiscs.nxs, dummy LOKI file\n",
    "    path_rawdata: filepath/to/ILL/rawdata\n",
    "    process_folder: filepath/to/folder/with/files/with/new/LOKI structure\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # convert numbers into strings\n",
    "    flist_num=[str(i).zfill(6) for i in scan_numbers]\n",
    "\n",
    "    for i in flist_num:\n",
    "        print(i, type(i))\n",
    "\n",
    "        # create new LOKI_basics.nxs\n",
    "        loki_file_creator(path_to_loki_file, loki_file)\n",
    "\n",
    "        # load old ILL data and extract data\n",
    "        data=load_one_spectro_file(i, path_rawdata)\n",
    "\n",
    "        # put extracted data into the new LOKI file structure by appending to loki_file\n",
    "        nurf_file_creator(loki_file, path_to_loki_file, data)\n",
    "\n",
    "        # rename LOKI_basic files to new file name\n",
    "        os.chdir(process_folder)\n",
    "        final_fname=i+'.nxs'\n",
    "        os.rename(loki_file, final_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 066150.nxs 066150\n",
      "065925 <class 'str'>\n",
      "065927 <class 'str'>\n",
      "065930 <class 'str'>\n",
      "065933 <class 'str'>\n",
      "065936 <class 'str'>\n",
      "065939 <class 'str'>\n",
      "065942 <class 'str'>\n",
      "065945 <class 'str'>\n",
      "065948 <class 'str'>\n",
      "065951 <class 'str'>\n",
      "065954 <class 'str'>\n",
      "065957 <class 'str'>\n",
      "065962 <class 'str'>\n",
      "065965 <class 'str'>\n",
      "065968 <class 'str'>\n",
      "065971 <class 'str'>\n",
      "065974 <class 'str'>\n",
      "065977 <class 'str'>\n",
      "065980 <class 'str'>\n",
      "065983 <class 'str'>\n",
      "065986 <class 'str'>\n",
      "065989 <class 'str'>\n",
      "065992 <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Location processed files\n",
    "process_folder='/Users/gudlo523/Library/CloudStorage/OneDrive-LundUniversity/UU/ILL_947_June_2021/212/d22/exp_9-13-947/processed/ess_version'\n",
    "\n",
    "# Location for LOKI_basic.nxs\n",
    "path_to_loki_file=process_folder\n",
    "\n",
    "# create LOKI_basic.nxs\n",
    "loki_file='LOKI_basic.nxs'\n",
    "loki_file_creator(path_to_loki_file, loki_file)\n",
    "\n",
    "# Location raw data files\n",
    "path_rawdata='/Users/gudlo523/Library/CloudStorage/OneDrive-LundUniversity/UU/ILL_947_June_2021/212/d22/exp_9-13-947/rawdata'\n",
    "\n",
    "\n",
    "os.chdir(path_rawdata)\n",
    "flist=os.listdir(path_rawdata)\n",
    "print(type(flist), flist[0], os.path.splitext(flist[0])[0])\n",
    "# get only filenumbers\n",
    "flist_num=[os.path.splitext(fnumber)[0] for fnumber in flist]\n",
    "\n",
    "# file numbers for ILL experiment 947\n",
    "#scan numbers in exp 5 \n",
    "exp5=[66017, 66020, 66023, 66026, 66029, 66032, 66034, 66037, 66040, 66043, 66046]\n",
    "#scan numbers in exp 6\n",
    "exp6= [66050, 66053, 66056, 66059, 66062, 66065, 66068, 66071, 66074, 66077, 66080]\n",
    "#scan_numbers=[exp5, exp6]\n",
    "\n",
    "# scan number in exp 7 and exp8\n",
    "exp7= [66083, 66086, 66089, 66092, 66095, 66098, 66101, 66104, 66107, 66110, 66113]\n",
    "exp8= [66116, 66119, 66122, 66125, 66128, 66131, 66134, 66137, 66140, 66143, 66146]\n",
    "\n",
    "#scan_numbers= [exp7, exp8]\n",
    "\n",
    "#for i in scan_numbers:\n",
    "#    convert_ill2loki(i, path_to_loki_file, loki_file, path_rawdata, process_folder)\n",
    "\n",
    "#exp2 \n",
    "exp2=[65925, 65927, 65930, 65933, 65936, 65939, 65942, 65945, 65948, 65951, 65954, 65957]\n",
    "exp3= [65962, 65965, 65968, 65971, 65974, 65977, 65980, 65983, 65986, 65989, 65992]\n",
    "\n",
    "scan_numbers= [exp2, exp3]\n",
    "\n",
    "for i in scan_numbers:\n",
    "    convert_ill2loki(i, path_to_loki_file, loki_file, path_rawdata, process_folder)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('scippneutron')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "410bfb39d85b3112e66f48ab3e53bb74ad7e7b3fb364f756ca860b5a3cf79ca2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
